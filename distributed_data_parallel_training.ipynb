{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e01b36a-8788-4322-bcf8-b9fe510a398a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1001424-1ee8-4593-a8b7-21abd2821f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e3afbf-7999-4e60-b5dd-b8d4229404bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d322cc-9e3b-4afb-a781-212fff9f3dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(rank, world_size):\n",
    "    \"\"\"Training function for each process\"\"\"\n",
    "    # Initialize process group for distributed training\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # Set device\n",
    "    torch.cuda.set_device(rank)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "    # Create dataset (random data for illustration)\n",
    "    dataset_size = 1000\n",
    "    x = torch.randn(dataset_size, 10)\n",
    "    y = torch.randint(0, 2, (dataset_size,))\n",
    "    dataset = TensorDataset(x, y)\n",
    "\n",
    "    # Use DistributedSampler to split data among processes\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "    # Create model, move to device, and wrap with DDP\n",
    "    model = SimpleModel().to(device)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(5):  # Train for 5 epochs\n",
    "        sampler.set_epoch(epoch)  # Ensure data shuffling consistency\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch+1} completed on rank {rank}\")\n",
    "\n",
    "    # Cleanup\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e6a38b9-05c1-45b2-b4ba-2fff43603439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Entry point\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()  # Number of GPUs available\n",
    "    if world_size < 2:\n",
    "        print(\"Need at least 2 GPUs for DistributedDataParallel\")\n",
    "        return\n",
    "\n",
    "    # Use multiprocessing to launch training processes\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2989e61f-b53e-4863-aa64-66a4fea4cc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "distributed_data_parallel_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
