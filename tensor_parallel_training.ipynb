{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "458b2112-2a7e-497b-be1f-315ddb044924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f24ddbc-955e-4111-bf6b-d2cba32294c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed.rpc as rpc\n",
    "from torch.distributed.rpc import RRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a1d1b8-0892-4721-9b62-6ed35dbece84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define model layers separately for parallel execution\n",
    "class ParallelLayer(nn.Module):\n",
    "    \"\"\"A single layer that will be distributed across processes\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ParallelLayer, self).__init__()\n",
    "        self.layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# Model split into multiple layers across different workers\n",
    "class TensorParallelModel(nn.Module):\n",
    "    def __init__(self, workers):\n",
    "        super(TensorParallelModel, self).__init__()\n",
    "        self.workers = workers  # List of workers (process names)\n",
    "\n",
    "        # Create Remote References (RRef) for each layer assigned to a worker\n",
    "        self.layer1_rref = rpc.remote(self.workers[0], ParallelLayer, args=(10, 50))\n",
    "        self.layer2_rref = rpc.remote(self.workers[1], ParallelLayer, args=(50, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass split across workers\n",
    "        x = self.layer1_rref.rpc_sync().forward(x)\n",
    "        x = self.layer2_rref.rpc_sync().forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2107f35b-4057-4f72-b03c-464f191582e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Worker function that initializes an RPC server\n",
    "def worker(rank, world_size):\n",
    "    rpc.init_rpc(f\"worker_{rank}\", rank=rank, world_size=world_size, backend=rpc.BackendType.TENSORPIPE)\n",
    "\n",
    "    # Wait for tasks\n",
    "    rpc.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba440fc-10c8-4a44-9df0-d7ad90b82bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Master function (main training loop)\n",
    "def main():\n",
    "    world_size = 3  # Two worker processes + one master process\n",
    "\n",
    "    # Start worker processes\n",
    "    mp.spawn(worker, args=(world_size,), nprocs=world_size - 1, join=False)\n",
    "\n",
    "    # Initialize RPC for master process\n",
    "    rpc.init_rpc(\"master\", rank=world_size - 1, world_size=world_size, backend=rpc.BackendType.TENSORPIPE)\n",
    "\n",
    "    # Create model with parallel layers assigned to workers\n",
    "    workers = [\"worker_0\", \"worker_1\"]\n",
    "    model = TensorParallelModel(workers)\n",
    "\n",
    "    # Create dummy dataset\n",
    "    x = torch.randn(32, 10)\n",
    "    y = torch.randint(0, 2, (32,))\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
    "\n",
    "    # Cleanup\n",
    "    rpc.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c828be-b997-4c80-9225-8ac83f4f05c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tensor_parallel_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
