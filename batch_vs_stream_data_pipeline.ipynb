{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032e5b6e-8f2f-4b7d-b9c5-05852d91bf93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44a786a-40e7-486e-82a5-b81df27e8e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import pandas as pd\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4347f2b2-c1ad-467d-928e-72196aef778e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    \"\"\"Reads data from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def transform(df):\n",
    "    \"\"\"Transforms data: filters out invalid rows and adds a computed column.\"\"\"\n",
    "    df = df[df['price'] > 0]  # Remove invalid prices\n",
    "    df['discounted_price'] = df['price'] * 0.9  # Apply 10% discount\n",
    "    return df\n",
    "\n",
    "def load(df, output_path):\n",
    "    \"\"\"Writes the transformed data to a new CSV file.\"\"\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "def batch_pipeline(input_file, output_file):\n",
    "    \"\"\"Orchestrates the batch pipeline.\"\"\"\n",
    "    df = extract(input_file)\n",
    "    df = transform(df)\n",
    "    load(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a2bb30-a265-4eb4-bcce-0c5d88985d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Producer function\n",
    "def run_producer():\n",
    "    \"\"\"Generates and sends transaction events continuously.\"\"\"\n",
    "\n",
    "    TOPIC_NAME = \"transactions\"\n",
    "    KAFKA_SERVER = \"localhost:9092\"\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_SERVER,\n",
    "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "    def generate_transaction():\n",
    "        \"\"\"Simulates a real-time transaction event.\"\"\"\n",
    "        return {\n",
    "            \"user_id\": random.randint(1, 100),\n",
    "            \"amount\": round(random.uniform(10, 500), 2),\n",
    "            \"timestamp\": time.time(),\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        transaction = generate_transaction()\n",
    "        producer.send(TOPIC_NAME, transaction)\n",
    "        print(f\"ðŸŸ¢ Sent: {transaction}\")\n",
    "        time.sleep(1)  # Simulate real-time events\n",
    "\n",
    "\n",
    "# Consumer function\n",
    "def run_consumer():\n",
    "    \"\"\"Consumes messages from Kafka topic and processes them.\"\"\"\n",
    "\n",
    "    TOPIC_NAME = \"transactions\"\n",
    "    KAFKA_SERVER = \"localhost:9092\"\n",
    "\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC_NAME,\n",
    "        bootstrap_servers=KAFKA_SERVER,\n",
    "        value_deserializer=lambda x: json.loads(x.decode(\"utf-8\")),\n",
    "    )\n",
    "\n",
    "    for message in consumer:\n",
    "        transaction = message.value\n",
    "        print(f\"ðŸ”µ Received: {transaction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c51601d-b7fa-46a8-9a1a-027822b88e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ########################\n",
    "    # Batch Processing\n",
    "    ########################\n",
    "    batch_pipeline('products.csv', 'processed_products.csv')\n",
    "    print(\"Batch processing completed.\")\n",
    "\n",
    "    ########################\n",
    "    # Stream Processing\n",
    "    ########################\n",
    "    # Running both Producer and Consumer in separate threads\n",
    "    producer_thread = threading.Thread(target=run_producer, daemon=True)\n",
    "    consumer_thread = threading.Thread(target=run_consumer, daemon=True)\n",
    "\n",
    "    producer_thread.start()\n",
    "    consumer_thread.start()\n",
    "\n",
    "    # Keep the main thread alive\n",
    "    while True:\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_vs_stream_data_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
